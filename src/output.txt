Project Path: src

Source Tree:

```
src
├── evaluators.py
├── models.py
├── output.txt
├── main.py
├── main.txt
├── requirements.txt
├── utils.py
├── __pycache__
│   ├── models.cpython-311.pyc
│   ├── main.cpython-311.pyc
│   ├── utils.cpython-311.pyc
│   ├── evaluators.cpython-311.pyc
│   └── __init__.cpython-311.pyc
├── __init__.py
└── requirements.in

```

`/home/setuc/gbb-demos/llm-evals/src/evaluators.py`:

```py
import re
import random
from typing import Dict, Callable
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM
from sentence_transformers import SentenceTransformer, util
from models import EvaluationResult, ProductReturnEvaluation

class LLMJudge:
    def __init__(self, model_name: str):
        self.model = AutoModelForCausalLM.from_pretrained(model_name, device_map="cpu")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.name = model_name

    def evaluate(self, query: str, context: str, response: str) -> EvaluationResult:
        prompt = f"""
            As an AI assistant, evaluate the following response.

            Query: {query}
            Context: {context}

            Criteria:
            1. Accuracy of information
            2. Completeness of the answer
            3. Relevance to the query
            4. Clarity and coherence

            Provide your evaluation in the following format:
            Score: <a number between 0 and 1>
            Explanation: <brief explanation>
            
            Response: {response}
            """

        inputs = self.tokenizer(prompt, return_tensors="pt")
        with torch.no_grad():
            outputs = self.model.generate(**inputs, max_new_tokens=100)
        
        result = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        score_match = re.search(r"Score:\s*([0-1](?:\.\d+)?)", result)

        score = float(score_match.group(1)) if score_match else 0.5
        # Add some randomness to the score
        score = max(0, min(1, score + random.uniform(-0.2, 0.2)))
        
        explanation_match = re.search(r"Explanation:\s*(.*)", result, re.DOTALL)
        if explanation_match:
            explanation = explanation_match.group(1).strip()
        else:
            explanation = "No explanation provided."

        
        return EvaluationResult(score, explanation)

class RandomJudge:
    def __init__(self, name: str):
        self.name = name

    def evaluate(self, query: str, context: str, response: str) -> EvaluationResult:
        score = random.uniform(0, 1)
        explanation = f"Random score generated: {score:.2f}"
        return EvaluationResult(score, explanation)

def semantic_similarity(response: str, context: str) -> EvaluationResult:
    model = SentenceTransformer('all-MiniLM-L6-v2')
    response_embedding = model.encode(response, convert_to_tensor=True)
    context_embedding = model.encode(context, convert_to_tensor=True)
    
    similarity = util.pytorch_cos_sim(response_embedding, context_embedding).item()
    return EvaluationResult(similarity, "Semantic similarity between response and context")

def evaluate_product_return_handling(
    response: str, 
    query: str, 
    context: str, 
    fact_checker: Callable, 
    toxicity_checker: Callable,
    llm_judges: Dict[str, LLMJudge]
) -> ProductReturnEvaluation:
    def score_with_explanation(score: float, explanation: str) -> EvaluationResult:
        return EvaluationResult(score, explanation)

    # Information Retrieval Accuracy
    product_mentioned = any(product in response.lower() for product in ["laptop", "smartphone", "book"])
    policy_mentioned = "return policy" in response.lower()
    purchase_history = re.search(r"\d{4}-\d{2}-\d{2}", response) is not None
    info_retrieval_score = (product_mentioned + policy_mentioned + purchase_history) / 3
    info_retrieval_explanation = f"Product mentioned: {product_mentioned}, Policy mentioned: {policy_mentioned}, Purchase history used: {purchase_history}"

    # Policy Application
    return_window_mentioned = any(term in response.lower() for term in ["30 days", "14 days", "7 days"])
    refund_calculation = "refund" in response.lower()
    exceptions_mentioned = "defective" in response.lower() if "defective" in query.lower() else True
    policy_application_score = (return_window_mentioned + refund_calculation + exceptions_mentioned) / 3
    policy_application_explanation = f"Return window mentioned: {return_window_mentioned}, Refund calculation: {refund_calculation}, Exceptions handled: {exceptions_mentioned}"

    # Response Completeness
    steps_provided = "steps" in response.lower() or re.search(r"\d+\)", response) is not None
    shipping_info = "shipping" in response.lower() or "drop off" in response.lower()
    timeline_explained = "5-7 business days" in response or "3-5 business days" in response
    completeness_score = (steps_provided + shipping_info + timeline_explained) / 3
    completeness_explanation = f"Steps provided: {steps_provided}, Shipping info: {shipping_info}, Timeline explained: {timeline_explained}"

    # Clarity and Coherence
    easy_to_understand = len(response.split()) < 150  # Assuming shorter responses are clearer
    appropriate_tone = "thank you" in response.lower() or "please" in response.lower()
    logical_order = steps_provided  # Reusing this as a proxy for logical order
    clarity_score = (easy_to_understand + appropriate_tone + logical_order) / 3
    clarity_explanation = f"Easy to understand: {easy_to_understand}, Appropriate tone: {appropriate_tone}, Logical order: {logical_order}"

    # Personalization
    customer_name_used = any(name in response for name in ["John", "Doe"])
    past_interaction = purchase_history  # Reusing this as a proxy for referencing past interactions
    specific_situation = product_mentioned and return_window_mentioned
    personalization_score = (customer_name_used + past_interaction + specific_situation) / 3
    personalization_explanation = f"Customer name used: {customer_name_used}, Past interaction referenced: {past_interaction}, Specific situation addressed: {specific_situation}"

    # Problem Resolution
    fully_addressed = all([product_mentioned, policy_mentioned, steps_provided, refund_calculation])
    follow_up_addressed = "if you have any questions" in response.lower()
    alternatives_provided = "alternative" in response.lower() or "option" in response.lower()
    resolution_score = (fully_addressed + follow_up_addressed + alternatives_provided) / 3
    resolution_explanation = f"Fully addressed: {fully_addressed}, Follow-up addressed: {follow_up_addressed}, Alternatives provided: {alternatives_provided}"

    # Factual Accuracy
    factual_accuracy_score = fact_checker(response)
    factual_accuracy_explanation = "Factual accuracy checked using external fact-checking model"

    # Safety
    safety_score = 1 - toxicity_checker(response)  # Invert the toxicity score
    safety_explanation = "Safety score based on toxicity check"

    # Semantic Similarity
    similarity_score = semantic_similarity(response, context)

    # LLM judges evaluation
    llm_judge_scores = {
        name: judge.evaluate(query, context, response)
        for name, judge in llm_judges.items()
    }

    return ProductReturnEvaluation(
        information_retrieval_accuracy=score_with_explanation(info_retrieval_score, info_retrieval_explanation),
        policy_application=score_with_explanation(policy_application_score, policy_application_explanation),
        response_completeness=score_with_explanation(completeness_score, completeness_explanation),
        clarity_and_coherence=score_with_explanation(clarity_score, clarity_explanation),
        personalization=score_with_explanation(personalization_score, personalization_explanation),
        problem_resolution=score_with_explanation(resolution_score, resolution_explanation),
        factual_accuracy=score_with_explanation(factual_accuracy_score, factual_accuracy_explanation),
        safety=score_with_explanation(safety_score, safety_explanation),
        semantic_similarity=similarity_score,
        llm_judge_scores=llm_judge_scores
    )
```

`/home/setuc/gbb-demos/llm-evals/src/models.py`:

```py
from dataclasses import dataclass, field
from typing import Dict

@dataclass
class EvaluationResult:
    score: float
    explanation: str = ""

@dataclass
class ProductReturnEvaluation:
    information_retrieval_accuracy: EvaluationResult
    policy_application: EvaluationResult
    response_completeness: EvaluationResult
    clarity_and_coherence: EvaluationResult
    personalization: EvaluationResult
    problem_resolution: EvaluationResult
    factual_accuracy: EvaluationResult
    safety: EvaluationResult
    semantic_similarity: EvaluationResult
    llm_judge_scores: Dict[str, EvaluationResult] = field(default_factory=dict)

    def overall_score(self) -> float:
        scores = [
            self.information_retrieval_accuracy.score,
            self.policy_application.score,
            self.response_completeness.score,
            self.clarity_and_coherence.score,
            self.personalization.score,
            self.problem_resolution.score,
            self.factual_accuracy.score,
            self.safety.score,
            self.semantic_similarity.score
        ] + [score.score for score in self.llm_judge_scores.values()]
        return sum(scores) / len(scores)

```

`/home/setuc/gbb-demos/llm-evals/src/main.py`:

```py
from typing import List, Dict, Tuple
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_recall,
)
from transformers import pipeline
from evaluators import LLMJudge, evaluate_product_return_handling, RandomJudge
from utils import generate_llm_response, calculate_iaa_metrics, calculate_judge_performance, print_evaluation_results

def run_evaluation(queries: List[str], contexts: List[str], gold_standard: List[float]) -> Tuple[Dict, List, Dict[str, float], Dict[str, Dict[str, float]]]:
    # Initialize models
    fact_checker = pipeline("text-classification", model="microsoft/deberta-base-mnli")
    toxicity_checker = pipeline("text-classification", model="unitary/toxic-bert")
    
    # Initialize LLM judges
    llm_judges = {
        "gpt2": LLMJudge("gpt2"),
        "distilgpt2": LLMJudge("distilgpt2"),
        "random_judge": RandomJudge("random_judge"),
        # Add more LLM judges as needed
    }

    # Generate responses
    responses = [generate_llm_response(query, context) for query, context in zip(queries, contexts)]

    # Prepare dataset for RAGAS
    dataset = Dataset.from_dict({
        "question": queries,
        "answer": responses,
        "contexts": [[context] for context in contexts],  # Each context needs to be a list
        "ground_truth": responses  # Using responses as ground truth for demonstration
    })

    # Define RAGAS evaluation metrics
    metrics = [faithfulness, answer_relevancy, context_recall]

    # Run RAGAS evaluation
    ragas_result = evaluate(dataset, metrics)

    # Run custom evaluation
    custom_evaluations = [
        evaluate_product_return_handling(
            response, query, context,
            lambda x: fact_checker(x)[0]['score'],
            lambda x: toxicity_checker(x)[0]['score'],
            llm_judges
        )
        for response, query, context in zip(responses, queries, contexts)
    ]

    # Calculate IAA metrics
    iaa_metrics = calculate_iaa_metrics(custom_evaluations)

    # Calculate judge performance
    judge_performance = calculate_judge_performance(custom_evaluations, gold_standard)

    return ragas_result, custom_evaluations, iaa_metrics, judge_performance

if __name__ == "__main__":
    # Sample data
    queries = [
        "I want to return the laptop I bought last month. What's the process?",
        "How do I return a defective smartphone?",
        "Can I get a refund for a book I ordered yesterday?"
    ]

    contexts = [
        "Customer John Doe purchased a laptop on 2023-05-15. Our return policy allows returns within 30 days of purchase for laptops. Refunds are processed within 5-7 business days after receiving the returned item.",
        "Smartphone return policy: Defective devices can be returned within 14 days of purchase. Customer must provide proof of purchase and describe the defect.",
        "Books can be returned within 7 days of delivery if unopened. Refunds are processed to the original payment method within 3-5 business days."
    ]

    # Define a mock gold standard for demonstration purposes
    gold_standard = [0.8, 0.7, 0.9]  # One score per query-response pair

    ragas_result, custom_evaluations, iaa_metrics, judge_performance = run_evaluation(queries, contexts, gold_standard)
    print_evaluation_results(ragas_result, custom_evaluations, iaa_metrics, judge_performance)
```

`/home/setuc/gbb-demos/llm-evals/src/utils.py`:

```py
from scipy import stats
import numpy as np
from sklearn.metrics import cohen_kappa_score
import krippendorff
from typing import List, Dict
from models import ProductReturnEvaluation

def calculate_iaa_metrics(evaluations: List[ProductReturnEvaluation]) -> Dict[str, float]:
    judge_scores = {judge: [eval.llm_judge_scores[judge].score for eval in evaluations] for judge in evaluations[0].llm_judge_scores}
    print(f'judge_scores: {judge_scores}')
    # Discretize scores into categories
    def discretize(score):
        if score < 0.33:
            return 0
        elif score < 0.67:
            return 1
        else:
            return 2

    discretized_scores = {judge: [discretize(score) for score in scores] for judge, scores in judge_scores.items()}
    
    # Cohen's Kappa (pairwise comparison)
    cohen_kappas = []
    judges = list(discretized_scores.keys())
    for i in range(len(judges)):
        for j in range(i+1, len(judges)):
            try:
                kappa = cohen_kappa_score(discretized_scores[judges[i]], discretized_scores[judges[j]])
                if not np.isnan(kappa):
                    cohen_kappas.append(kappa)
            except ValueError:
                pass
    
    # Krippendorff's Alpha
    try:
        reliability_data = np.array(list(judge_scores.values())).T
        alpha = krippendorff.alpha(reliability_data=reliability_data.tolist(), level_of_measurement='interval')
    except ValueError:
        alpha = None
    
    return {
        "avg_cohen_kappa": np.mean(cohen_kappas) if cohen_kappas else None,
        "krippendorff_alpha": alpha
    }

def calculate_judge_performance(evaluations: List[ProductReturnEvaluation], gold_standard: List[float]) -> Dict[str, Dict[str, float]]:
    judge_scores = {judge: [eval.llm_judge_scores[judge].score for eval in evaluations] for judge in evaluations[0].llm_judge_scores}
    
    performance = {}
    for judge, scores in judge_scores.items():
        precision = np.mean([1 if abs(s - g) < 0.1 else 0 for s, g in zip(scores, gold_standard)])
        recall = np.mean([1 if abs(s - g) < 0.1 else 0 for s, g in zip(scores, gold_standard)])
        f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0
        agreement = np.mean([1 if abs(s - g) < 0.1 else 0 for s, g in zip(scores, gold_standard)])
        z_score = stats.zscore(scores)
        
        performance[judge] = {
            "precision": precision,
            "recall": recall,
            "f1_score": f1,
            "percent_agreement": agreement,
            "avg_z_score": np.mean(z_score)
        }
    
    return performance

def generate_llm_response(query: str, context: str) -> str:
    # This is a mock function. In a real scenario, this would call your actual LLM.
    return "Thank you for contacting us about returning your product. Based on our records, you purchased a laptop on 2023-05-15. Our return policy allows returns within 30 days of purchase. To initiate the return process, please follow these steps: 1) Pack the laptop in its original packaging. 2) Include all accessories. 3) Print the return label from your account. 4) Drop off the package at any authorized shipping location. Once we receive the item, we'll process your refund within 5-7 business days. If you have any questions, please don't hesitate to ask."

def print_evaluation_results(ragas_result: Dict, custom_evaluations: List[ProductReturnEvaluation], iaa_metrics: Dict[str, float], judge_performance: Dict[str, Dict[str, float]]):
    print("RAGAS Evaluation Results:")
    print(ragas_result)

    print("\nCustom Evaluation Results:")
    for i, eval in enumerate(custom_evaluations):
        print(f"\nEvaluation for Response {i+1}:")
        print(f"Information Retrieval Accuracy: {eval.information_retrieval_accuracy.score:.2f}")
        print(f"  Explanation: {eval.information_retrieval_accuracy.explanation}")
        print(f"Policy Application: {eval.policy_application.score:.2f}")
        print(f"  Explanation: {eval.policy_application.explanation}")
        print(f"Response Completeness: {eval.response_completeness.score:.2f}")
        print(f"  Explanation: {eval.response_completeness.explanation}")
        print(f"Clarity and Coherence: {eval.clarity_and_coherence.score:.2f}")
        print(f"  Explanation: {eval.clarity_and_coherence.explanation}")
        print(f"Personalization: {eval.personalization.score:.2f}")
        print(f"  Explanation: {eval.personalization.explanation}")
        print(f"Problem Resolution: {eval.problem_resolution.score:.2f}")
        print(f"  Explanation: {eval.problem_resolution.explanation}")
        print(f"Factual Accuracy: {eval.factual_accuracy.score:.2f}")
        print(f"  Explanation: {eval.factual_accuracy.explanation}")
        print(f"Safety: {eval.safety.score:.2f}")
        print(f"  Explanation: {eval.safety.explanation}")
        print(f"Semantic Similarity: {eval.semantic_similarity.score:.2f}")
        print(f"  Explanation: {eval.semantic_similarity.explanation}")
        print("\nLLM Judge Scores:")
        for judge_name, judge_score in eval.llm_judge_scores.items():
            print(f"  {judge_name}: {judge_score.score:.2f}")
            print(f"    Explanation: {judge_score.explanation}")
        print(f"Overall Score: {eval.overall_score():.2f}")

    print("\nInter-Annotator Agreement Metrics:")
    for metric, value in iaa_metrics.items():
        if value is not None:
            print(f"{metric}: {value:.4f}")
        else:
            print(f"{metric}: Unable to calculate")

    print("\nJudge Performance Metrics:")
    for judge, metrics in judge_performance.items():
        print(f"\n{judge}:")
        for metric, value in metrics.items():
            print(f"  {metric}: {value:.4f}")
            
```