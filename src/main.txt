import re
from typing import List, Dict, Tuple, Callable
from dataclasses import dataclass, field
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_relevancy,
    context_recall,
    harmfulness,
)
from datasets import Dataset
import numpy as np
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
import torch
from sentence_transformers import SentenceTransformer, util
from sklearn.metrics import cohen_kappa_score, fleiss_kappa
import krippendorff
from scipy import stats

@dataclass
class EvaluationResult:
    score: float
    explanation: str = ""

@dataclass
class ProductReturnEvaluation:
    information_retrieval_accuracy: EvaluationResult
    policy_application: EvaluationResult
    response_completeness: EvaluationResult
    clarity_and_coherence: EvaluationResult
    personalization: EvaluationResult
    problem_resolution: EvaluationResult
    factual_accuracy: EvaluationResult
    safety: EvaluationResult
    semantic_similarity: EvaluationResult
    llm_judge_scores: Dict[str, EvaluationResult] = field(default_factory=dict)

    def overall_score(self) -> float:
        scores = [
            self.information_retrieval_accuracy.score,
            self.policy_application.score,
            self.response_completeness.score,
            self.clarity_and_coherence.score,
            self.personalization.score,
            self.problem_resolution.score,
            self.factual_accuracy.score,
            self.safety.score,
            self.semantic_similarity.score
        ] + [score.score for score in self.llm_judge_scores.values()]
        return np.mean(scores)

class LLMJudge:
    def __init__(self, model_name: str):
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.name = model_name

    def evaluate(self, query: str, context: str, response: str) -> EvaluationResult:
        prompt = f"""
        Given the following:
        Query: {query}
        Context: {context}
        Response: {response}

        Evaluate the response based on the following criteria:
        1. Accuracy of information
        2. Completeness of the answer
        3. Relevance to the query
        4. Clarity and coherence

        Provide a score between 0 and 1, where 1 is the best possible score, and a brief explanation.
        """

        inputs = self.tokenizer(prompt, return_tensors="pt")
        with torch.no_grad():
            outputs = self.model.generate(**inputs, max_new_tokens=100)
        
        result = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        score_match = re.search(r"Score: (\d+\.\d+)", result)
        score = float(score_match.group(1)) if score_match else 0.5
        explanation = result.split("Explanation:")[-1].strip() if "Explanation:" in result else result
        
        return EvaluationResult(score, explanation)

def semantic_similarity(response: str, context: str) -> EvaluationResult:
    model = SentenceTransformer('all-MiniLM-L6-v2')
    response_embedding = model.encode(response, convert_to_tensor=True)
    context_embedding = model.encode(context, convert_to_tensor=True)
    
    similarity = util.pytorch_cos_sim(response_embedding, context_embedding).item()
    return EvaluationResult(similarity, "Semantic similarity between response and context")

def evaluate_product_return_handling(
    response: str, 
    query: str, 
    context: str, 
    fact_checker: Callable, 
    toxicity_checker: Callable,
    llm_judges: Dict[str, LLMJudge]
) -> ProductReturnEvaluation:
    def score_with_explanation(score: float, explanation: str) -> EvaluationResult:
        return EvaluationResult(score, explanation)

    # Information Retrieval Accuracy
    product_mentioned = any(product in response.lower() for product in ["laptop", "smartphone", "book"])
    policy_mentioned = "return policy" in response.lower()
    purchase_history = re.search(r"\d{4}-\d{2}-\d{2}", response) is not None
    info_retrieval_score = (product_mentioned + policy_mentioned + purchase_history) / 3
    info_retrieval_explanation = f"Product mentioned: {product_mentioned}, Policy mentioned: {policy_mentioned}, Purchase history used: {purchase_history}"

    # Policy Application
    return_window_mentioned = any(term in response.lower() for term in ["30 days", "14 days", "7 days"])
    refund_calculation = "refund" in response.lower()
    exceptions_mentioned = "defective" in response.lower() if "defective" in query.lower() else True
    policy_application_score = (return_window_mentioned + refund_calculation + exceptions_mentioned) / 3
    policy_application_explanation = f"Return window mentioned: {return_window_mentioned}, Refund calculation: {refund_calculation}, Exceptions handled: {exceptions_mentioned}"

    # Response Completeness
    steps_provided = "steps" in response.lower() or re.search(r"\d+\)", response) is not None
    shipping_info = "shipping" in response.lower() or "drop off" in response.lower()
    timeline_explained = "5-7 business days" in response or "3-5 business days" in response
    completeness_score = (steps_provided + shipping_info + timeline_explained) / 3
    completeness_explanation = f"Steps provided: {steps_provided}, Shipping info: {shipping_info}, Timeline explained: {timeline_explained}"

    # Clarity and Coherence
    easy_to_understand = len(response.split()) < 150  # Assuming shorter responses are clearer
    appropriate_tone = "thank you" in response.lower() or "please" in response.lower()
    logical_order = steps_provided  # Reusing this as a proxy for logical order
    clarity_score = (easy_to_understand + appropriate_tone + logical_order) / 3
    clarity_explanation = f"Easy to understand: {easy_to_understand}, Appropriate tone: {appropriate_tone}, Logical order: {logical_order}"

    # Personalization
    customer_name_used = any(name in response for name in ["John", "Doe"])
    past_interaction = purchase_history  # Reusing this as a proxy for referencing past interactions
    specific_situation = product_mentioned and return_window_mentioned
    personalization_score = (customer_name_used + past_interaction + specific_situation) / 3
    personalization_explanation = f"Customer name used: {customer_name_used}, Past interaction referenced: {past_interaction}, Specific situation addressed: {specific_situation}"

    # Problem Resolution
    fully_addressed = all([product_mentioned, policy_mentioned, steps_provided, refund_calculation])
    follow_up_addressed = "if you have any questions" in response.lower()
    alternatives_provided = "alternative" in response.lower() or "option" in response.lower()
    resolution_score = (fully_addressed + follow_up_addressed + alternatives_provided) / 3
    resolution_explanation = f"Fully addressed: {fully_addressed}, Follow-up addressed: {follow_up_addressed}, Alternatives provided: {alternatives_provided}"

    # Factual Accuracy
    factual_accuracy_score = fact_checker(response)
    factual_accuracy_explanation = "Factual accuracy checked using external fact-checking model"

    # Safety
    safety_score = 1 - toxicity_checker(response)  # Invert the toxicity score
    safety_explanation = "Safety score based on toxicity check"

    # Semantic Similarity
    similarity_score = semantic_similarity(response, context)

    # LLM judges evaluation
    llm_judge_scores = {
        name: judge.evaluate(query, context, response)
        for name, judge in llm_judges.items()
    }

    return ProductReturnEvaluation(
        information_retrieval_accuracy=score_with_explanation(info_retrieval_score, info_retrieval_explanation),
        policy_application=score_with_explanation(policy_application_score, policy_application_explanation),
        response_completeness=score_with_explanation(completeness_score, completeness_explanation),
        clarity_and_coherence=score_with_explanation(clarity_score, clarity_explanation),
        personalization=score_with_explanation(personalization_score, personalization_explanation),
        problem_resolution=score_with_explanation(resolution_score, resolution_explanation),
        factual_accuracy=score_with_explanation(factual_accuracy_score, factual_accuracy_explanation),
        safety=score_with_explanation(safety_score, safety_explanation),
        semantic_similarity=similarity_score,
        llm_judge_scores=llm_judge_scores
    )

def calculate_iaa_metrics(evaluations: List[ProductReturnEvaluation]) -> Dict[str, float]:
    judge_scores = {judge: [eval.llm_judge_scores[judge].score for eval in evaluations] for judge in evaluations[0].llm_judge_scores}
    
    # Cohen's Kappa (pairwise comparison)
    cohen_kappas = []
    judges = list(judge_scores.keys())
    for i in range(len(judges)):
        for j in range(i+1, len(judges)):
            kappa = cohen_kappa_score(judge_scores[judges[i]], judge_scores[judges[j]])
            cohen_kappas.append(kappa)
    
    # Fleiss' Kappa
    data = np.array(list(judge_scores.values())).T
    fleiss = fleiss_kappa(data)
    
    # Krippendorff's Alpha
    alpha = krippendorff.alpha(reliability_data=list(judge_scores.values()))
    
    return {
        "avg_cohen_kappa": np.mean(cohen_kappas),
        "fleiss_kappa": fleiss,
        "krippendorff_alpha": alpha
    }

def calculate_judge_performance(evaluations: List[ProductReturnEvaluation], gold_standard: List[float]) -> Dict[str, Dict[str, float]]:
    judge_scores = {judge: [eval.llm_judge_scores[judge].score for eval in evaluations] for judge in evaluations[0].llm_judge_scores}
    
    performance = {}
    for judge, scores in judge_scores.items():
        precision = np.mean([1 if abs(s - g) < 0.1 else 0 for s, g in zip(scores, gold_standard)])
        recall = np.mean([1 if abs(s - g) < 0.1 else 0 for s, g in zip(scores, gold_standard)])
        f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0
        agreement = np.mean([1 if abs(s - g) < 0.1 else 0 for s, g in zip(scores, gold_standard)])
        z_score = stats.zscore(scores)
        
        performance[judge] = {
            "precision": precision,
            "recall": recall,
            "f1_score": f1,
            "percent_agreement": agreement,
            "avg_z_score": np.mean(z_score)
        }
    
    return performance

def generate_llm_response(query: str, context: str) -> str:
    # This is a mock function. In a real scenario, this would call your actual LLM.
    return "Thank you for contacting us about returning your product. Based on our records, you purchased a laptop on 2023-05-15. Our return policy allows returns within 30 days of purchase. To initiate the return process, please follow these steps: 1) Pack the laptop in its original packaging. 2) Include all accessories. 3) Print the return label from your account. 4) Drop off the package at any authorized shipping location. Once we receive the item, we'll process your refund within 5-7 business days. If you have any questions, please don't hesitate to ask."

def run_evaluation(queries: List[str], contexts: List[str], gold_standard: List[float]) -> Tuple[Dict, List[ProductReturnEvaluation], Dict[str, float], Dict[str, Dict[str, float]]]:
    # Initialize models
    fact_checker = pipeline("text-classification", model="microsoft/deberta-base-mnli")
    toxicity_checker = pipeline("text-classification", model="unitary/toxic-bert")
    
    # Initialize LLM judges
    llm_judges = {
        "gpt2": LLMJudge("gpt2"),
        "distilgpt2": LLMJudge("distilgpt2"),
        # Add more LLM judges as needed
    }

    # Generate responses
    responses = [generate_llm_response(query, context) for query, context in zip(queries, contexts)]

    # Prepare dataset for RAGAS
    dataset = Dataset.from_dict({
        "query": queries,
        "context": contexts,
        "response": responses
    })

    # Define RAGAS evaluation metrics
    metrics = [faithfulness, answer_relevancy, context_relevancy, context_recall, harmfulness]

    # Run RAGAS evaluation
    ragas_result = evaluate(dataset, metrics)

    # Run custom evaluation
    custom_evaluations = [
        evaluate_product_return_handling(
            response, query, context,
            lambda x: fact_checker(x)[0]['score'],
            lambda x: toxicity_checker(x)[0]['score'],
            llm_judges
        )
        for response, query, context in zip(responses, queries, contexts)
    ]

    # Calculate IAA metrics
    iaa_metrics = calculate_iaa_metrics(custom_evaluations)

    # Calculate judge performance
    judge_performance = calculate_judge_performance(custom_evaluations, gold_standard)

    return ragas_result, custom_evaluations, iaa_metrics, judge_performance

def print_evaluation_results(ragas_result: Dict, custom_evaluations: List[ProductReturnEvaluation], iaa_metrics: Dict[str, float], judge_performance: Dict[str, Dict[str, float]]):
    print("RAGAS Evaluation Results:")
    print(ragas_result)

    print("\nCustom Evaluation Results:")
    for i, eval in enumerate(custom_evaluations):
        print(f"\nEvaluation for Response {i+1}:")
        print(f"Information Retrieval Accuracy: {eval.information_retrieval_accuracy.score:.2f}")
        print(f"  Explanation: {eval.information_retrieval_accuracy.explanation}")
        print(f"Policy Application: {eval.policy_application.score:.2f}")
        print(f"  Explanation: {eval.policy_application.explanation}")
        print(f"Response Completeness: {eval.response_completeness.score:.2f}")
        print(f"  Explanation: {eval.response_completeness.explanation}")
        print(f"Clarity and Coherence: {eval.clarity_and_coherence.score:.2f}")
        print(f"  Explanation: {eval.clarity_and_coherence.explanation}")
        print(f"Personalization: {eval.personalization.score:.2f}")
        print(f"  Explanation: {eval.personalization.explanation}")
        print(f"Problem Resolution: {eval.problem_resolution.score:.2f}")
        print(f"  Explanation: {eval.problem_resolution.explanation}")
        print(f"Factual Accuracy: {eval.factual_accuracy.score:.2f}")
        print(f"  Explanation: {eval.factual_accuracy.explanation}")
        print(f"Safety: {eval.safety.score:.2f}")
        print(f"  Explanation: {eval.safety.explanation}")
        print(f"Semantic Similarity: {eval.semantic_similarity.score:.2f}")
        print(f"  Explanation: {eval.semantic_similarity.explanation}")
        print("\nLLM Judge Scores:")
        for judge_name, judge_score in eval.llm_judge_scores.items():
            print(f"  {judge_name}: {judge_score.score:.2f}")
            print(f"    Explanation: {judge_score.explanation}")
        print(f"Overall Score: {eval.overall_score():.2f}")

    print("\nInter-Annotator Agreement Metrics:")
    for metric, value in iaa_metrics.items():
        print(f"{metric}: {value:.4f}")

    print("\nJudge Performance Metrics:")
    for judge, metrics in judge_performance.items():
        print(f"\n{judge}:")
        for metric, value in metrics.items():
            print(f"  {metric}: {value:.4f}")

if __name__ == "__main__":
    # Sample data
    queries = [
        "I want to return the laptop I bought last month. What's the process?",
        "How do I return a defective smartphone?",
        "Can I get a refund for a book I ordered yesterday?"
    ]

    contexts = [
        "Customer John Doe purchased a laptop on 2023-05-15. Our return policy allows returns within 30 days of purchase for laptops. Refunds are processed within 5-7 business days after receiving the returned item.",
        "Smartphone return policy: Defective devices can be returned within 14 days of purchase. Customer must provide proof of purchase and describe the defect.",
        "Books can be returned within 7 days of delivery if unopened. Refunds are processed to the original payment method within 3-5 business days."
    ]

    # Define a mock gold standard for demonstration purposes
    gold_standard = [0.8, 0.7, 0.9]  # One score per query-response pair

    ragas_result, custom_evaluations, iaa_metrics, judge_performance = run_evaluation(queries, contexts, gold_standard)
    print_evaluation_results(ragas_result, custom_evaluations, iaa_metrics, judge_performance)